{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dogs vs Cats\n",
    "## Kaggle Dataset의 전부를 이용한 개, 고양이 구분\n",
    "### Dog Image: 12,500개, Cat Image: 12,500개, 총 25,000개\n",
    "### 출처: [pontoregende GitHub](https://github.com/pontorezende/Dogs-vs-Cats-Redux-with-CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "import cv2, os, random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed 값 설정\n",
    "seed = 2020\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='dogs-vs-cats/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## used for resize and in our model\n",
    "ROW, COL = 96, 96\n",
    "\n",
    "dogs, cats = [], []\n",
    "y_dogs, y_cats = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definition to load all our dog images\n",
    "def load_dogs():\n",
    "    print('Loading all dog images\\n')\n",
    "    dog_path = os.path.join(path, 'dog*')\n",
    "    for dog_img in glob(dog_path):\n",
    "        dog = cv2.imread(dog_img)\n",
    "        dog = cv2.cvtColor(dog, cv2.COLOR_BGR2GRAY)\n",
    "        dog = cv2.resize(dog, (ROW, COL))\n",
    "        dog = image.img_to_array(dog)\n",
    "        dogs.append(dog)\n",
    "    print('All dog images loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definition to load all our cat images\n",
    "def load_cats():\n",
    "    print('Loading all cat images\\n')\n",
    "    cat_path = os.path.join(path, 'cat*')\n",
    "    for cat_img in glob(cat_path):\n",
    "        cat = cv2.imread(cat_img)\n",
    "        cat = cv2.cvtColor(cat, cv2.COLOR_BGR2GRAY)\n",
    "        cat = cv2.resize(cat, (ROW, COL))\n",
    "        cat = image.img_to_array(cat)\n",
    "        cats.append(cat)\n",
    "    print('All cat images loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all dog images\n",
      "\n",
      "All dog images loaded\n"
     ]
    }
   ],
   "source": [
    "load_dogs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all cat images\n",
      "\n",
      "All cat images loaded\n"
     ]
    }
   ],
   "source": [
    "load_cats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['dog', 'cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## in case we want to see if our images was saved correctly in arrays we can use those codes\n",
    "def show_dogs():\n",
    "    plt.figure(figsize=(12,8))    \n",
    "    for i in range(5):\n",
    "        plt.subplot(1, 5, i+1)\n",
    "        img = image.array_to_img(random.choice(dogs))\n",
    "        plt.imshow(img)\n",
    "        \n",
    "        plt.axis('off')\n",
    "        plt.title('Supposed to be a {}'.format(classes[0]))        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cats():\n",
    "    plt.figure(figsize=(12,8))\n",
    "    for i in range(5):\n",
    "        plt.subplot(1, 5, i+1)\n",
    "        img = image.array_to_img(random.choice(cats))\n",
    "        plt.imshow(img)\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.title('Supposed to be a {}'.format(classes[1]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## just change the labels for 0 and  1\n",
    "y_dogs = [1 for item in enumerate(dogs)]\n",
    "y_cats = [0 for item in enumerate(cats)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## converting everything to Numpy array to fit in our model\n",
    "## them creating a X and target file like we used to see\n",
    "## in Machine and Deep Learning models\n",
    "dogs = np.asarray(dogs).astype('float32') / 255\n",
    "cats = np.asarray(cats).astype('float32') / 255\n",
    "y_dogs = np.asarray(y_dogs).astype('int32')\n",
    "y_cats = np.asarray(y_cats).astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((dogs,cats), axis=0)\n",
    "y = np.concatenate((y_dogs, y_cats), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_CHANNEL = 1\n",
    "BATCH_SIZE = 128\n",
    "N_EPOCH = 10\n",
    "VERBOSE = 2\n",
    "VALIDAION_SPLIT = .2\n",
    "OPTIM = Adam()\n",
    "N_CLASSES = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## One-Hot Encoding\n",
    "y = keras.utils.to_categorical(y, N_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ckkim\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "The model was created by following config:\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 96, 96, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 96, 96, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 48, 48, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 48, 48, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 36864)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               18874880  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 18,940,898\n",
      "Trainable params: 18,940,898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Here is our model as a CNN\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), padding='same', input_shape=(ROW, COL, IMG_CHANNEL), activation='relu'),\n",
    "    Conv2D(32, (3,3), padding='same', activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Dropout(.25),\n",
    "    Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "    Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Dropout(.25),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(.5),\n",
    "    Dense(N_CLASSES, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=OPTIM, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to save checkpoint to use latter\n",
    "modelpath = 'model_checkpoint/dogs_vs_cats_redux_checkpoint.h5'\n",
    "checkpoint = ModelCheckpoint(filepath=modelpath)\n",
    "model.fit(X, y, batch_size=BATCH_SIZE, epochs=N_EPOCH, \n",
    "          validation_split=VALIDAION_SPLIT,\n",
    "          verbose=VERBOSE, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL ACCURACY\n",
      "acc: 95.304%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X, y, verbose=2)\n",
    "print('MODEL ACCURACY: %.5f' % scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON saved\n",
      ".h5 saved\n",
      "Weights saved in .h5 file\n"
     ]
    }
   ],
   "source": [
    "model_name = 'Dogs-and-Cats-12500-CNN'\n",
    "## saving architecture\n",
    "model_json = model.to_json()\n",
    "open(model_name+'.json', 'w').write(model_json)\n",
    "print('JSON saved')\n",
    "\n",
    "## and the weights learned by our deep network on the training set\n",
    "model.save(model_name+'.h5', overwrite=True)\n",
    "print('.h5 saved')\n",
    "\n",
    "model.save_weights(model_name+'_weights.h5', overwrite=True)\n",
    "print('Weights saved in .h5 file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
